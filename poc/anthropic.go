// provider.go â€” OpenAI-compatible provider
// æ”¯æŒ MiniMaxã€DeepSeekã€Kimiã€é€šä¹‰åƒé—®ç­‰æ‰€æœ‰ OpenAI å…¼å®¹æ¨¡å‹
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"strings"

	"github.com/openai/openai-go"
	"github.com/openai/openai-go/option"
	"github.com/openai/openai-go/shared"
)

// Message æ˜¯å¯¹è¯å†å²ä¸­çš„ä¸€æ¡æ¶ˆæ¯
type Message struct {
	Role    string    // "user" | "assistant"
	Content []Content // æ”¯æŒå¤š content block
}

// Content æ˜¯æ¶ˆæ¯ä¸­çš„ä¸€ä¸ªå†…å®¹å—
type Content struct {
	Type       string          // "text" | "tool_use" | "tool_result"
	Text       string          // Type=="text"
	ToolUseID  string          // Type=="tool_use" / "tool_result"
	ToolName   string          // Type=="tool_use"
	ToolInput  json.RawMessage // Type=="tool_use"
	ToolResult string          // Type=="tool_result"
	IsError    bool
}

// ToolCall ä»£è¡¨ LLM è¯·æ±‚æ‰§è¡Œçš„å•ä¸ªå·¥å…·è°ƒç”¨
type ToolCall struct {
	ID    string
	Name  string
	Input json.RawMessage
}

// StreamResult æ˜¯ä¸€è½® streaming å®Œæˆåçš„ç»“æœ
type StreamResult struct {
	TextContent string
	ToolCalls   []ToolCall
	StopReason  string // "stop" | "tool_calls" | "length"
}

// LLMClient å°è£… OpenAI-compatible API
type LLMClient struct {
	client   openai.Client // å€¼ç±»å‹ï¼Œä¸æ˜¯æŒ‡é’ˆ
	model    string
	registry *ToolRegistry
}

func NewLLMClient(registry *ToolRegistry) *LLMClient {
	apiKey := os.Getenv("LLM_API_KEY")
	baseURL := os.Getenv("LLM_BASE_URL")
	model := os.Getenv("LLM_MODEL")

	if apiKey == "" {
		fmt.Fprintln(os.Stderr, "error: LLM_API_KEY is not set")
		fmt.Fprintln(os.Stderr, "")
		fmt.Fprintln(os.Stderr, "MiniMax ç¤ºä¾‹:")
		fmt.Fprintln(os.Stderr, "  export LLM_API_KEY=your-minimax-key")
		fmt.Fprintln(os.Stderr, "  export LLM_BASE_URL=https://api.minimax.chat/v1")
		fmt.Fprintln(os.Stderr, "  export LLM_MODEL=MiniMax-Text-01")
		os.Exit(1)
	}

	opts := []option.RequestOption{option.WithAPIKey(apiKey)}
	if baseURL != "" {
		opts = append(opts, option.WithBaseURL(baseURL))
	}
	if model == "" {
		model = "gpt-4o-mini"
	}

	return &LLMClient{
		client:   openai.NewClient(opts...),
		model:    model,
		registry: registry,
	}
}

// buildAPIMessages å°†å†…éƒ¨ Message è½¬æ¢ä¸º OpenAI API æ ¼å¼
func buildAPIMessages(history []Message) []openai.ChatCompletionMessageParamUnion {
	var params []openai.ChatCompletionMessageParamUnion

	for _, msg := range history {
		switch msg.Role {
		case "user":
			// tool_result å’Œæ™®é€šæ–‡æœ¬åˆ†å¼€å¤„ç†
			for _, c := range msg.Content {
				switch c.Type {
				case "text":
					params = append(params, openai.UserMessage(c.Text))
				case "tool_result":
					params = append(params, openai.ToolMessage(c.ToolResult, c.ToolUseID))
				}
			}

		case "assistant":
			var text string
			var toolCalls []openai.ChatCompletionMessageToolCallParam

			for _, c := range msg.Content {
				switch c.Type {
				case "text":
					text = c.Text
				case "tool_use":
					toolCalls = append(toolCalls, openai.ChatCompletionMessageToolCallParam{
						ID:   c.ToolUseID,
						Type: "function",
						Function: openai.ChatCompletionMessageToolCallFunctionParam{
							Name:      c.ToolName,
							Arguments: string(c.ToolInput),
						},
					})
				}
			}

			assistant := openai.ChatCompletionAssistantMessageParam{
				Content:   openai.ChatCompletionAssistantMessageParamContentUnion{OfString: openai.String(text)},
				ToolCalls: toolCalls,
			}
			params = append(params, openai.ChatCompletionMessageParamUnion{OfAssistant: &assistant})
		}
	}
	return params
}

// buildTools å°† ToolRegistry è½¬æ¢ä¸º OpenAI tools æ ¼å¼
func buildTools(registry *ToolRegistry) []openai.ChatCompletionToolParam {
	var tools []openai.ChatCompletionToolParam
	for _, schema := range registry.ToSchema() {
		props := schema["input_schema"].(map[string]any)["properties"]
		tools = append(tools, openai.ChatCompletionToolParam{
			Type: "function",
			Function: shared.FunctionDefinitionParam{
				Name:        schema["name"].(string),
				Description: openai.String(schema["description"].(string)),
				Parameters: shared.FunctionParameters{
					"type":       "object",
					"properties": props,
				},
			},
		})
	}
	return tools
}

// Chat å‘èµ·ä¸€æ¬¡ streaming å¯¹è¯ï¼Œå¤„ç† tool use çŠ¶æ€æœºã€‚
//
// OpenAI streaming tool use çš„å…³é”®å·®å¼‚ï¼ˆå¯¹æ¯” Anthropicï¼‰ï¼š
//   - tool call é€šè¿‡ delta.tool_calls[] è¿”å›
//   - æ¯ä¸ª tool call æœ‰ index å­—æ®µåŒºåˆ†ï¼ˆåŒä¸€å“åº”ä¸­å¤šä¸ª tool call æ—¶ï¼‰
//   - id å’Œ name åªåœ¨è¯¥ tool call çš„ç¬¬ä¸€ä¸ª delta ä¸­å‡ºç°
//   - arguments æ˜¯å¢é‡ JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦æ‹¼æ¥
func (c *LLMClient) Chat(ctx context.Context, history []Message, systemPrompt string) (*StreamResult, error) {
	// æ„é€ æ¶ˆæ¯åˆ—è¡¨
	var msgs []openai.ChatCompletionMessageParamUnion
	if systemPrompt != "" {
		msgs = append(msgs, openai.SystemMessage(systemPrompt))
	}
	msgs = append(msgs, buildAPIMessages(history)...)

	// å‘èµ· streaming è¯·æ±‚
	stream := c.client.Chat.Completions.NewStreaming(ctx, openai.ChatCompletionNewParams{
		Model:    openai.ChatModel(c.model),
		Messages: msgs,
		Tools:    buildTools(c.registry),
	})

	// â”€â”€ çŠ¶æ€æœºå˜é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

	var textBuf strings.Builder

	type pendingCall struct {
		id      string
		name    string
		jsonBuf strings.Builder
	}
	pending := make(map[int]*pendingCall) // key: tool call index
	var callOrder []int
	var stopReason string

	// â”€â”€ é€ chunk å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
	for stream.Next() {
		chunk := stream.Current()

		if len(chunk.Choices) == 0 {
			continue
		}
		choice := chunk.Choices[0]

		if string(choice.FinishReason) != "" {
			stopReason = string(choice.FinishReason)
		}

		delta := choice.Delta

		// æ–‡æœ¬å¢é‡
		if delta.Content != "" {
			fmt.Print(delta.Content)
			textBuf.WriteString(delta.Content)
		}

		// Tool call å¢é‡
		for _, tc := range delta.ToolCalls {
			idx := int(tc.Index)

			if _, exists := pending[idx]; !exists {
				pending[idx] = &pendingCall{}
				callOrder = append(callOrder, idx)
			}
			pc := pending[idx]

			if tc.ID != "" {
				pc.id = tc.ID
			}
			if tc.Function.Name != "" {
				pc.name = tc.Function.Name
				fmt.Printf("\nğŸ”§ Tool: %s ", tc.Function.Name)
			}
			if tc.Function.Arguments != "" {
				pc.jsonBuf.WriteString(tc.Function.Arguments)
			}
		}
	}

	if err := stream.Err(); err != nil {
		return nil, fmt.Errorf("streaming error: %w", err)
	}

	// æ‰“å°æ¯ä¸ª tool call çš„å‚æ•°æ‘˜è¦
	for _, idx := range callOrder {
		pc := pending[idx]
		fmt.Printf("â† %s\n", truncate(pc.jsonBuf.String(), 80))
	}

	// ç»„è£…ç»“æœ
	result := &StreamResult{
		TextContent: textBuf.String(),
		StopReason:  stopReason,
	}
	for _, idx := range callOrder {
		pc := pending[idx]
		inputJSON := pc.jsonBuf.String()
		if inputJSON == "" {
			inputJSON = "{}"
		}
		result.ToolCalls = append(result.ToolCalls, ToolCall{
			ID:    pc.id,
			Name:  pc.name,
			Input: json.RawMessage(inputJSON),
		})
	}

	return result, nil
}

// truncate æˆªæ–­å­—ç¬¦ä¸²ç”¨äºç»ˆç«¯å±•ç¤º
func truncate(s string, max int) string {
	if len(s) <= max {
		return s
	}
	return s[:max] + "..."
}
